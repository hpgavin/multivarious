# Multi-Dimensional Fitting with lm.py

## This implementation of levenberg-marquard works for ANY Number of Variables without modification



No matter how many independent variables you have, the independent variables are in a **2D array**:

```python
# 1 variable (just time t)
t.shape = (100,)           # 1D array - special case

# 2 variables (x(t), y(t)
t.shape = (200, 2)         # 2D array

# 5 variables ( p(t), q(t), r(t), s(t), u(t) )
t.shape = (300, 5)         # 2D array

```

**The pattern:**

- Rows = number of observations 
- Columns = number of independent variables
- Always `t.ndim = 2` (except single-variable case)

### Current Validation (Already Perfect!)

```python
# In lm.py, lines 221-228:
if t.ndim == 1:
    if len(t) != n_points:
        raise ValueError(...)
elif t.ndim == 2:
    if t.shape[0] != n_points:  # âœ“ Only checks number of points
        raise ValueError(...)    # âœ“ Doesn't care about t.shape[1]!
else:
    raise ValueError(...)
```

**This validation:**

- âœ“ Checks `t.shape[0]` matches data points
- âœ“ **Doesn't limit** `t.shape[1]` (number of variables)
- âœ“ Works for 2, 10, 1000 variables automatically!

## Algorithm Components (All Dimension-Agnostic)

### 1. Function Evaluation

```python
# Algorithm just calls:
y_hat = func(t, coeffs)

# Your function extracts variables:
def func_10d(t, coeffs):
    p, q, r, s, u, v, w, x, y, z = [t[:, i] for i in range(10)]
    # Build model from all variables
    return model_expression
```

### 2. Jacobian Computation

```python
# Finite differences perturb COEFFICIENTS, not variables
for i, coeff in enumerate(coeffs):
    coeffs_perturbed[i] += delta
    y_plus = func(t, coeffs_perturbed)   # t unchanged!
    J[:, i] = (y_plus - y) / delta
```

**Key:** Jacobian perturbs *coefficients*, not *independent variables*. Works for any `t` shape!

### 3. Residuals

```python
# Always produces 1D vector
residuals = y_data - func(t, coeffs)
# Shape: (n_points,) regardless of how many variables in t
```

### 4. Chi-Squared

```python
chi_sq = residuals.T @ (residuals * weight)
# Scalar value, doesn't depend on t.shape
```

## Practical Examples

### 2D Fitting (Already Works)

```python
# Example 4: (x, y) â†’ z
n_points = 200
x = np.random.rand(n_points)
y = np.random.rand(n_points)
t = np.column_stack([x, y])  # Shape: (200, 2)

def func_2d(t, coeffs):
    x, y = t[:, 0], t[:, 1]
    return coeffs[0] * x**coeffs[1] + (1-coeffs[0]) * y**coeffs[1]

result = levenberg_marquardt(func_2d, coeffs_init, t, z_data)
```

### 5D Fitting (Tested and Works!)

```python
# (p, q, r, s, u) â†’ z
n_points = 300
p = np.random.rand(n_points)
q = np.random.rand(n_points)
r = np.random.rand(n_points)
s = np.random.rand(n_points)
u = np.random.rand(n_points)
t = np.column_stack([p, q, r, s, u])  # Shape: (300, 5)

def func_5d(t, coeffs):
    p, q, r, s, u = [t[:, i] for i in range(5)]
    return (coeffs[0]*p**2 + coeffs[1]*q**2 + coeffs[2]*r + 
            coeffs[3]*np.sin(s) + coeffs[4]*np.exp(u))

result = levenberg_marquardt(func_5d, coeffs_init, t, z_data)
```

### 10D Fitting (Will Work!)

```python
# (p, q, r, s, u, v, w, x, y, z) â†’ output
n_points = 1000
n_vars = 10

# Generate random data for all 10 variables
vars_data = [np.random.rand(n_points) for _ in range(n_vars)]
t = np.column_stack(vars_data)  # Shape: (1000, 10)

def func_10d(t, coeffs):
    # Extract all 10 variables
    p, q, r, s, u, v, w, x, y, z = [t[:, i] for i in range(10)]

    # Your complex model here
    return (coeffs[0]*p + coeffs[1]*q**2 + coeffs[2]*np.sin(r) + 
            coeffs[3]*s*u + coeffs[4]*np.exp(v) + ...)

result = levenberg_marquardt(func_10d, coeffs_init, t, output_data)
```

## Only Limitation: Visualization

The algorithm works perfectly, but **visualization gets tricky**:

### 1D â†’ 2D Plot (Easy)

```python
plt.plot(t, y_data, 'o')
plt.plot(t, y_fit, '-')
```

### 2D â†’ 3D Plot (Doable)

```python
ax = fig.add_subplot(111, projection='3d')
ax.scatter(x, y, z_data)
ax.scatter(x, y, z_fit)
```

### 5D â†’ ??? (Challenging)

Can't visualize 5D space directly! Options:

**Option 1: Pairwise Residual Plots**

```python
# Plot residuals vs each variable separately
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
for i, var_name in enumerate(['p', 'q', 'r', 's', 'u']):
    axes.flat[i].scatter(t[:, i], residuals)
    axes.flat[i].set_xlabel(var_name)
    axes.flat[i].set_ylabel('Residuals')
```

**Option 2: Predicted vs Actual**

```python
# Single plot showing fit quality
plt.scatter(y_data, y_fit)
plt.plot([min, max], [min, max], 'r--', label='Perfect fit')
plt.xlabel('Actual')
plt.ylabel('Predicted')
```

**Option 3: Convergence History Only**

```python
# Focus on optimization progress, not spatial visualization
plt.semilogy(cvg_history[:, 0], cvg_history[:, n_coeffs+1])
plt.xlabel('Function Evaluations')
plt.ylabel('Ï‡Â²')
```

## Computational Considerations

### Memory Usage

```python
# Storage requirements scale with:
# - n_points (number of data points)
# - n_coeffs (number of parameters to fit)
# NOT with n_vars (number of independent variables)!

# Jacobian: (n_points, n_coeffs)
# For n_points=1000, n_coeffs=10:
# J.size = 10,000 floats â‰ˆ 80 KB
```

### Speed

```python
# Bottleneck is function evaluation in Jacobian
# Each Jacobian computation requires:
# - Central differences: 2*n_coeffs function calls
# - One-sided: n_coeffs function calls

# With 10 coefficients, central differences:
# â†’ 20 function evaluations per Jacobian update

# n_vars doesn't affect speed (assuming func is well-written)
```

## Best Practices for High-Dimensional Fitting

### 1. Vectorize Your Function

```python
# GOOD: Vectorized (fast)
def func_10d(t, coeffs):
    result = np.zeros(len(t))
    for i in range(10):
        result += coeffs[i] * t[:, i]**2
    return result

# BETTER: Fully vectorized (faster)
def func_10d(t, coeffs):
    return np.sum(coeffs * t**2, axis=1)
```

### 2. Scale Your Variables

```python
# If variables have very different ranges:
# p âˆˆ [0, 1], q âˆˆ [0, 1000], r âˆˆ [0, 0.01]

# Normalize to similar scales
t_scaled = (t - t.mean(axis=0)) / t.std(axis=0)

# Use scaled version in fitting
result = levenberg_marquardt(func, coeffs_init, t_scaled, y_data)
```

### 3. Check for Multicollinearity

```python
# If variables are highly correlated, fitting can be unstable
correlation_matrix = np.corrcoef(t.T)
print(correlation_matrix)

# Look for correlations > 0.9 between variables
# Consider removing redundant variables
```

### 4. Regularization for Many Parameters

```python
# If n_coeffs is large (say >20), consider:
# - L1 regularization (LASSO) for sparse models
# - L2 regularization (Ridge) for stability
# - These aren't built into lm.py but can be added to chi_sq
```

## Testing Framework

Here's how to test your multi-dimensional model:

```python
def test_nd_fitting(n_vars=10, n_coeffs=10, n_points=500):
    """
    Generic test for n-dimensional fitting.
    """
    # Generate random data
    t = np.random.rand(n_points, n_vars)
    coeffs_true = np.random.randn(n_coeffs)

    # Simple linear model for testing
    def func(t_in, c):
        # Use first n_coeffs variables
        return np.sum(c[:, None] * t_in[:, :n_coeffs].T, axis=0)

    # Generate noisy data
    y_data = func(t, coeffs_true) + 0.1*np.random.randn(n_points)

    # Fit
    coeffs_init = np.ones(n_coeffs)
    result = levenberg_marquardt(func, coeffs_init, t, y_data)

    # Check convergence
    error = np.linalg.norm(result.coefficients - coeffs_true)
    print(f"{n_vars}D fitting: error = {error:.6f}")

    return error < 1.0  # Success criterion

# Test increasing dimensions
for n in [2, 5, 10, 20, 50]:
    success = test_nd_fitting(n_vars=n, n_coeffs=min(n, 10))
    print(f"  {'âœ“' if success else 'âœ—'} {n} variables")
```

## Summary

| Dimensions   | Works? | Visualization               | Notes                   |
| ------------ | ------ | --------------------------- | ----------------------- |
| 1D (t â†’ y)   | âœ“      | Easy (2D plot)              | Standard case           |
| 2D (x,y â†’ z) | âœ“      | Good (3D plot)              | Example 4               |
| 3D-5D        | âœ“      | Moderate (pairwise plots)   | Tested with 5D example  |
| 6D-20D       | âœ“      | Hard (residual plots)       | Algorithm works fine    |
| 20D+         | âœ“      | Very hard (statistics only) | Check multicollinearity |

**Bottom line:** 

- âœ“ Algorithm: Works perfectly for ANY number of variables
- âœ“ Validation: Already correct
- âœ“ Performance: Scales well
- âš  Visualization: Gets harder with more dimensions (but that's a general problem, not specific to LM)

**No code changes needed!** Just format your data as `t.shape = (n_points, n_vars)` and it works! ðŸŽ‰
